# 자연어 처리(NLP)

- NLU(Understanding)
- NLG(Generating)

## Bag-of-words

데이터로 가지고 있는 문장을 단어 단위로 나누는 Tokenization 과정을 거칩니다. 그 다음Token들 중에서 겹치는 단어들을 제외한 Unique 한 단어들을 모아줍니다. Unique한 단어들의 개수만큼의 차원을 가진 원핫 벡터를 만들고 이것을 각 단어들과 맵핑해줍니다. 

이렇게 맵핑한다면 그 결과는 단어의 의미와 관계없이 모든 단어가 동일한 관계를 가지는 형태로 단어의 벡터 표현형을 설정하는 것 입니다. 

준비된 벡터를 가지고 문장을 표현해 봅시다. 

여기서 Bag-of-words 벡터가 어떤 뜻인지 알수 있습니다. 먼저 우리가 가진 각 단어별로 가방을 준비한다고 해봅시다. 이제 벡터로 바꾸고 싶은 특정 문장에서 나타난 단어들은 순차적으로 해당하는 가방에 넣어줍니다. 그리고 첫번째 가방([1,0,0,0,0,0,0]인 단어가 들어있는 가방)부터 가방속의 단어의 개수를 세어 최종벡터를 만들수 있습니다.  

## Naive Bayes Classifier

임의의 문장을 d라고 하고 문장을 분류하는 클래스로 c가 있다고 한다면 $P(d \cap c)$ 즉, d가 c일 확률을 구한다고 하면 조건부확률을 이용해 다음과 같이 표현할 수 있다. $$P(d | c)P(c)=P(w_1, w_2, ..., w_n|c)P(c) \to P(c)\prod _{w_i \in W}P(w_i|c)$$ 
문장을 단어 단위로 쪼개고 클래스와 단어 독립인 관계라고 가정하면 확률의 곱으로 표현할 수 있게됩니다.
클래스에 따른 단어의 확률을 구할때에는 이미 각 클래스로 구분된 문장들에서 추출한 단어들 중 해당 단어가 몇번 나왔는지로 구하게 됩니다. 
각 클래스 마다 해당 클래스일 확률을 구하고 가장 확률이 높은 클래스를 문장의 클래스로 선택하는 것입니다. 이러한 방법은 단점이 존재하는데 예를 들어 어떤 단어가 A 클래스의 문장에서 쓰이지 않았다면 그 확률은 0이 될 것입니다. 그렇다면 테스트로 그 단어가 들어간 문장이 들어왔을때 그 문장을 구성하는 다른 단어들의 확률이 아무리 높더라도 A 클래스일 확률이 0이 된다는 문제가 있습니다.

## Word Embedding

각각의 단어를 특정한 차원의 벡터로 표현하는 방법입니다. 여기서는 Bag-of-words와는 다르게 비슷한 의미를 가지는 단어는 공간상에서 가까운 위치로 맵핑됩니다. 일반적으로 word embedding 알고리즘은 텍스트 데이터와 차원 수를 입력으로 받아 학습을 진행한 뒤에 데이터의 포함된 단어의 최적의 촤표값을 아웃풋으로 제공합니다.

### word2vec
비슷한 의미를 가지는 단어가 벡터공간에서 비슷한 위치를 갖게하는 방법으로 다음을 제시합니다. 한 단어의 의미를 파악할 때 그 단어가 포함된 문장에서 그 단어 주변에 등장하는 단어를 사용하는 것입니다. 예를 들어 "cat"이라는 단어를 생각할 때 $P(w|"cat")$ 즉, cat 근처에 올 수 있는 단어의 확률을 계산하여 모델이 이를 학습합니다.<br>
좀 더 구체적으로 살펴봅시다.
주어진 문장 데이터를 tokenization 과정을 통해 vocabulary 즉, 단어 모음을 만들어줍니다.(이하 vocab이라 칭함) 단어 모음의 크기만큼 차원을 가지는 원 핫 벡터에 vocab의 단어를 맵핑합니다. 이제 전처리는 끝이 났습니다.
다음으로 "i study math" 이 문장이 모델의 데이터라고 가정하고 예를 들어 설명하겠습니다. 주변 단어의 추출을 위해 sliding window 방법을 사용합니다. 만약 윈도우의 크기가 3이라고 한다면 다음 4개의 입력-출력 쌍을 만들 수 있습니다. (study|i), (i|study), (math|study), (study|math) 이들을 학습데이터로 사용합니다. 학습데이터를 사용하여 예측 task를 수행하는 2 layers 모델을 만듭니다. 이때 입출력 노드 수는 vocab의 크기인 3이 되고 hidden layer의 노드 수는 하이퍼파라미터로 임베딩 차원으로 설정합니다. 모델에서 output 값이 나오기 전에 softmax연산을 통해 3개의 노드 값을 확률 분포 값으로 바꿔줍니다. softmax 연산의 결과와 ground truth 사이의 차이를 줄이는 softmax loss 를 사용하여 가중치 학습을 진행합니다.<br>
여기서 입력 벡터가 원 핫 벡터라는 것에 집중해본다면 첫번째 연산은 입력 단어에 대한 W1 즉, 입력을 입베딩 차원의 벡터로 변환하는 행렬의 column 벡터를 뽑아오는 것이라고 생각해볼 수 있습니다. 실제 모델 구현시에서도 실제 연산 대신에 열 벡터를 가져오는 형태를 가집니다. 또한 출력 벡터(ground truth)도 원 핫 벡터이므로 두번쨰 연산을 거치고 softmax에 들어가기 전 즉, logit 값을 1에 해당하는 것은 $\infty$으로 0에 해당하는 것은 $-\infty$으로 만들어야 합니다. 이때 첫번째 연산을 거쳐 나온 벡터 즉, 입력단어에 대응하는 W1의 벡터와 W2 즉, 임베딩 벡터를 출력으로 변환하는 행렬과 연산을 W2의 행벡터들과의 내적연산이라고 생각한다면 내적에 의한 유사도를 logit값으로 얻게 됩니다. 이 내적에 의한 유사도를 생각해볼때 출력단어에 대응하는 W2의 행벡터와의 내적에서는 유사도를 최대로 만들어야하고 그 외의 행벡터와는 유사도를 최소로 만들어야합니다. 이 과정에서 의미가 유사한 단어가 비슷한 위치에 임베딩된다는 것입니다. 부연 설명하자면 출력단어에 대응하는 W2의 행벡터와 입력단어에 대응하는 W1의 열벡터가 유사해집니다. 예를 들어 (orange|eat), (apple|eat) 와 같은 학습 데이터가 존재한다면 W1의 orange와 apple에 해당하는 벡터가 학습 중에 W2의 eat에 해당하는 벡터와 유사해지려고 하면서 결국 둘이 유사해지는 것입니다. <br>
이러한 방법으로 학습한 word2vec의 임베딩은 의미를 잘 담고 있다고 알려져 있습니다. (예시, 한복 - 대한민국 + 일본 = 기모노) 또한 단어들이 주어지고 가장 관련없는 단어를 찾는 Intrusion detection task를 수행할 수 있습니다. 
### GloVe
이 모델은 입력과 출력 단어 쌍이 윈도우에서 중복하여 등장하는 횟수를 미리 계산하여 이를 loss함수에 추가하였습니다. word2vec에서는 예를 들어 (math|study)쌍이 여러번 등장하면 이를 그만큼 학습하여 내적값이 점점 커지도록 했다면 GloVe는 아예 중복하여 등장한 횟수에 로그를 취한 값을 ground truth로 제공하여 중복된 계산을 줄여준다는 장점이 있습니다. 이 과정으로 학습이 빨라지고 적은 데이터에서도 잘 동작합니다. 하지만 성능은 word2vec과 비슷하다고 합니다. <br><br>
word2vec또는 GloVe의 사이트에 접속하면 학습에 사용한 코드와 단어 임베딩 결과를 다운받을 수 있습니다. 파일에는 예를 들어 (유니크 단어 개수 40k) x (임베딩 차원 50),  (유니크 단어 개수 40k) x (임베딩 차원 100)... 과 같은 행렬이 존재합니다.
